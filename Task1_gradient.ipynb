{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(15000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 15 seconds\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 5\n",
    "%autosave 15\n",
    "\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(f, df, start_point, a_start=10, a_modify=lambda a: a * 0.999, normalize=True,\n",
    "                    steps=1000000, l=-1000000000, r=1000000000, min_gradient=0.00001):\n",
    "    \n",
    "    if (type(start_point) != int):\n",
    "        p = start_point.copy()\n",
    "    else:\n",
    "        p = start_point\n",
    "    a = a_start\n",
    "    for _ in range(steps):\n",
    "        gradient = df(p)\n",
    "        norm = np.linalg.norm(gradient)\n",
    "        if (norm < min_gradient):\n",
    "            break\n",
    "        if (normalize):\n",
    "            gradient = gradient / norm\n",
    "        p = p - a * gradient\n",
    "        p = np.clip(p, l, r)\n",
    "        a = a_modify(a)\n",
    "    return p\n",
    "\n",
    "def monte_carlo(f, df, count, dims, a_start=10, a_modify=lambda a: a * 0.999, normalize=True,\n",
    "                steps=1000000, l=-1000000000, r=1000000000, min_gradient=0.000001):\n",
    "    points = []\n",
    "    values = []\n",
    "    for i in range(count):\n",
    "        rand_point = np.random.random(dims) * (r - l) + l\n",
    "        t = gradient_descent(f, df, rand_point, a_start=a_start, a_modify=a_modify, normalize=normalize,\n",
    "                            steps=steps, l=l, r=r, min_gradient=min_gradient)\n",
    "        points.append(t)\n",
    "        values.append(f(t))\n",
    "    return points[np.argmin(values)]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 rozenbrok gradient: [ 1.00000001  1.        ] 4.97201543071e-14\n"
     ]
    }
   ],
   "source": [
    "f = lambda p: (1 - p[0]) ** 2 + 100 * (p[1] - p[0] ** 2) ** 2\n",
    "df = lambda p: np.array([(2 * (-1 + p[0] + 200 * p[0] ** 3 - 200 * p[0] * p[1])), (-200 * (p[0] ** 2 - p[1]))])\n",
    "p = np.array([100.0, -2311.0])\n",
    "\n",
    "res = gradient_descent(f, df, p, a_start=100, a_modify=lambda a: a * 0.999, normalize=True, steps=100000)\n",
    "print(\"2 rozenbrok gradient:\",res, f(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 rozenbrok monte carlo: [ 1.00000016  0.99999982] 2.50261906766e-11\n"
     ]
    }
   ],
   "source": [
    "f = lambda p: (1 - p[0]) ** 2 + 100 * (p[1] - p[0] ** 2) ** 2\n",
    "df = lambda p: np.array([(2 * (-1 + p[0] + 200 * p[0] ** 3 - 200 * p[0] * p[1])), (-200 * (p[0] ** 2 - p[1]))])\n",
    "p = np.array([12000.0, -2311.0])\n",
    "\n",
    "res = monte_carlo(f, df, 10, 2, a_start=10, a_modify=lambda a: a * 0.999, normalize=True, steps=10000,\n",
    "                  l=-1000, r=1000)\n",
    "print(\"2 rozenbrok monte carlo:\", res, f(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 rozenbrok gradient: [ 0.99999985  0.99999998  0.99999969  1.00000006  0.99999962  1.00000005\n",
      "  0.99999961  0.99999988  0.99999944  0.99999902] 8.0219057704e-12\n"
     ]
    }
   ],
   "source": [
    "def fn(point):\n",
    "    res = 0\n",
    "    for i in range(point.size - 1):\n",
    "        res += (1 - point[i]) ** 2 + (point[i + 1] - point[i] ** 2) ** 2\n",
    "    return res\n",
    "\n",
    "def dfn(point, eps=0.0000001):\n",
    "    res = []\n",
    "    p = point.copy()\n",
    "    dims = point.size\n",
    "    for i in range(dims):\n",
    "        p[i] += eps\n",
    "        t = (fn(p) - fn(point)) / eps\n",
    "        res.append(t)\n",
    "        p[i] -= eps\n",
    "    return res\n",
    "\n",
    "p = np.array([12000.0, -2311.0, 4.0, 2.4, 123.2, 45.2, -123, -123, 34, 0])\n",
    "\n",
    "res = gradient_descent(fn, dfn, p, a_start=100, a_modify=lambda a: a * 0.999, normalize=True, steps=100000)\n",
    "\n",
    "print(\"10 rozenbrok gradient:\",res, f(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "??: [ 0.99988923  0.99977802] 1.22903925219e-08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def optimal(f, df, point):\n",
    "    def funcopt(a):\n",
    "        grad = df(point)\n",
    "        grad = grad / np.linalg.norm(grad)\n",
    "        return f(point - a * grad)\n",
    "    return funcopt\n",
    "\n",
    "def doptimal(f, fd, point, eps=0.0000001):\n",
    "    def funcdopt(a):\n",
    "        return (optimal(f, df, point)(a + eps) - optimal(f, df, point)(a)) / eps\n",
    "    return funcdopt\n",
    "\n",
    "def gradient_descent_optimal(f, df, start_point, min_a=0.000000001, max_a=10, a_steps=100,normalize=True,\n",
    "                    steps=1000000, l=-1000000000, r=1000000000, min_gradient=0.00001):\n",
    "    \n",
    "    if (type(start_point) != int):\n",
    "        p = start_point.copy()\n",
    "    else:\n",
    "        p = start_point\n",
    "    for _ in range(steps):\n",
    "        gradient = df(p)\n",
    "        norm = np.linalg.norm(gradient)\n",
    "        if (norm < min_gradient):\n",
    "            break\n",
    "        if (normalize):\n",
    "            gradient = gradient / norm\n",
    "        la = min_a\n",
    "        ra = max_a\n",
    "        for _ in range(a_steps):\n",
    "            a1 = la + (ra - la) / 3\n",
    "            a2 = ra - (ra - la) / 3\n",
    "            if f(p - a1 * gradient) < f(p - a2 * gradient):\n",
    "                ra = a2\n",
    "            else:\n",
    "                la = a1\n",
    "        p = p - la * gradient\n",
    "        p = np.clip(p, l, r)\n",
    "    return p\n",
    "\n",
    "f = lambda p: (1 - p[0]) ** 2 + 100 * (p[1] - p[0] ** 2) ** 2\n",
    "df = lambda p: np.array([(2 * (-1 + p[0] + 200 * p[0] ** 3 - 200 * p[0] * p[1])), (-200 * (p[0] ** 2 - p[1]))])\n",
    "p = np.array([100.0, -2311.0])\n",
    "\n",
    "res = gradient_descent_optimal(f, df, p, min_a=0.000000001, max_a=0.001, a_steps=20, normalize=False,\n",
    "                               steps=20000, l=-1000000000, r=1000000000)\n",
    "print(\"??:\",res, f(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
